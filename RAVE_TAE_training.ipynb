{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1983,"status":"ok","timestamp":1682441976376,"user":{"displayName":"Pablo Navarro","userId":"03312151943936382336"},"user_tz":-120},"id":"xkt2DJCeW3YC","outputId":"56d114ba-d3f4-44e9-949e-d78478cdbe08"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Found GPU at: \n"]}],"source":["import sys\n","import json\n","import os.path\n","from os import path\n","import numpy as np\n","import tensorflow as tf\n","from google.colab import drive\n","from keras import backend as K\n","from keras.models import Model\n","from keras.layers import Input, Dense, Lambda\n","from keras.initializers import RandomUniform, Constant\n","from keras.optimizers import RMSprop\n","from keras.constraints import unit_norm\n","from keras import regularizers\n","from keras.callbacks import Callback\n","from keras.losses import mean_squared_error\n","from tensorflow.python.framework.ops import disable_eager_execution\n","\n","disable_eager_execution()\n","\n","drive.mount('/content/gdrive')\n","\n","device_name = tf.test.gpu_device_name()\n","#if device_name != '/device:GPU:0':\n","#  raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))\n"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":258,"status":"ok","timestamp":1682441979762,"user":{"displayName":"Pablo Navarro","userId":"03312151943936382336"},"user_tz":-120},"id":"0FwY9jbuYHCJ"},"outputs":[],"source":["def data_prep(system_name, number_trajs, predictive_step, input_dir, whiten_data, num_ops_used):\n","    \"\"\" \n","    Read the input trajectory files. Prepare X, X_dt trajectory and corresponding re-weighting factors. Whiten if requested.\n","        \n","        Parameters\n","        ----------\n","        system_name : string\n","            Name of the system.\n","            \n","        number_trajs : int\n","            Number of trajectories.\n","        \n","        predictive_step : int\n","            Predictive time delay (in number of samples).\n","\n","        input_dir : str\n","            Input dir where .npy files are saved\n","          \n","        whiten_data : bool \n","            Whether to whiten the data or not (setting mean 0 and variance 1)\n","\n","        Returns\n","        -------\n","        X : np.array\n","            present trajectory.\n","         \n","        X_dt : np.array\n","            future trajectory.\n","            \n","        W1 : np.array\n","            re-weighting factors in objective function before P(X_t | \\chi )\n","            \n","        W2 : np.array\n","            re-weighting factors in objective function before P(X | \\chi )\n","    \"\"\"\n","    \n","    # For each trajectory\n","    for j in range(number_trajs):\n","\n","        # trajectory npy file names\n","        traj_file_name = input_dir + 'x_'+system_name+'_%i.npy'%j   # present trajectory of the shape n*d, where n is the MD steps and d is the number of order parameters (PLUMED output)\n","        w_file_name    = input_dir + 'w_'+system_name+'_%i.npy'%j   # weights correspond to trajectory in \"traj_file_name\". Calculated by exp(beta*V)   \n","\n","        # If time delay = 0\n","        if predictive_step==0:\n","\n","            # Load trajectory data\n","            x = np.load(traj_file_name)\n","            x_dt = x[:,:]      \n","\n","            # Load weights\n","            w1 = np.load(w_file_name)   \n","            w2 = np.zeros(np.shape(w1))      \n","\n","        # If time delay =! 0\n","        else:\n","\n","            # Load trajectory data\n","            x = np.load(traj_file_name)\n","\n","            # Save data, shifted \"time delay\" wrt each other\n","            x_dt = x[predictive_step: , :]  # x_dt (advanced dt with respect to X)\n","            x = x[:-predictive_step, :]     # x\n","\n","            # Load weights\n","            w = np.load(w_file_name)\n","\n","            # Save corresponding weights\n","            w_x = w[:-predictive_step] # weights for x\n","            w_y = w[predictive_step:]  # weights for x_dt   \n","\n","            # Will be used in the loss function \n","            w1 = ( w_x * w_y )**0.5\n","            w2 =  w_x**0.5*( w_x**0.5 - w_y**0.5)\n","\n","        # 2. For subsequent trajectories\n","        try:\n","            # Append trajectory data to full dataset\n","            X = np.append(X, x, axis = 0)\n","            X_dt = np.append(X_dt, x_dt, axis = 0)\n","\n","            # Append trajectory weights to all weights\n","            W1 = np.append(W1, w1, axis = 0)\n","            W2 = np.append(W2, w2, axis = 0)\n","\n","        # 1. For the first trajectory\n","        except:\n","            X = x\n","            X_dt = x_dt\n","\n","            W1 = w1\n","            W2 = w2\n","\n","    normalization_factor = np.sum(W1)/len(W1)  \n","\n","    W1 /= normalization_factor\n","    W2 /= normalization_factor    \n","\n","    print('length of data: %i'%np.shape(X)[0])\n","    print('number of order parameters: %i'%np.shape(X)[1])\n","    print('min re-weighting factor: %f'%np.min(W1))\n","    print('max re-weighting factor: %f'%np.max(W1))  \n","\n","    if whiten_data:\n","        X, scaling_factors = scaling(X) \n","        X_dt -= np.mean(X_dt, axis=0)\n","        X_dt /= scaling_factors\n","    else:\n","        scaling_factors = np.ones(num_ops_used) \n","\n","    return X, X_dt, W1, W2, scaling_factors\n","\n","def random_pick(x, x_dt, w1, w2, training_len, validation_len):\n","    \"\"\" \n","    Finds training set: ramdomly pick (x, x_dt) pair from data set of size \"training_len\"  \n","    Finds validation set: ramdomly pick (x, x_dt) pair from data set of size \"validation_len\" \n","\n","        Parameters\n","        ----------\n","        x : np.array\n","            present trajectory.\n","         \n","        x_dt : np.array\n","            future trajectory.\n","            \n","        w1 : np.array\n","            re-weighting factors in objective function before P(X_t | \\chi )\n","            \n","        w2 : np.array\n","            re-weighting factors in objective function before P(X | \\chi )\n","            \n","        training_len: int\n","            length of the return training set\n","\n","        validation_len: int\n","            length of the return validation set  \n","                \n","        Returns\n","        -------\n","\n","        train_x, vali_x : np.array\n","            randomly selected data points from trajectory for training and validation sets.\n","         \n","        train_xdt, vali_xdt : np.array\n","            train_x and vali_x shifted dt\n","\n","        train_w1, vali_w1 : np.array\n","            corresponding re-weighting factors in objective function before the decoder P(X_t | \\chi ) (?)\n","            \n","        train_w2, vali_w2 : np.array\n","            corresponding re-weighting factors in objective function before encoder P(X | \\chi ) (?)\n","\n","    \"\"\"   \n","    # Find total number of samples per OP\n","    num_samples = np.shape(x)[0]\n","\n","    # Check we have enough samples \n","    if ((training_len+validation_len)>num_samples):\n","      print(\"ERROR: Chosen training and validation lengths exceeds total amount of used samples\")\n","      sys.exit()\n","\n","    # Create indices for all samples\n","    indices = np.arange(num_samples) \n","\n","    # Shuffle indices  \n","    np.random.shuffle(indices)\n","\n","    # First \"training_len\" samples will be used for training\n","    train_indices = indices[:training_len]\n","\n","    # Next \"validation_len\" samples will be used for validation\n","    val_indices = indices[training_len:training_len+validation_len]\n","\n","    # Find samples for training set, all columns (all OPs)\n","    train_x = x[train_indices, :]\n","    train_xdt = x_dt[train_indices, :]\n","\n","    # Find samples for validation set, all columns (all OPs)\n","    vali_x = x[val_indices, :]\n","    vali_xdt = x_dt[val_indices, :]\n","\n","    # Find weights for training set, all columns (all OPs)\n","    train_w1 = w1[train_indices]\n","    train_w2 = w2[train_indices]\n","\n","    # Find weights for validation set, all columns (all OPs)\n","    vali_w1 = w1[val_indices]\n","    vali_w2 = w2[val_indices]\n","\n","    print('{} data points are used in this training'.format(training_len))\n","    \n","    return train_x, train_xdt, train_w1, train_w2, vali_x, vali_xdt, vali_w1, vali_w2\n","    \n","def scaling(x):\n","    \"\"\" make order parameters with mean 0 and variance 1\n","        return new order parameter and scaling factors\n","        \n","        Parameters\n","        ----------\n","        x : np.array\n","            order parameters\n","            \n","        Returns\n","        ----------\n","        x : np.array\n","            order parameters after rescaling\n","        \n","        std_x : np.array\n","            rescaling factors of each OPs\n","              \n","     \"\"\" \n","   \n","    x = x-np.mean(x, axis =0)\n","    std_x = np.std(x, axis =0)\n","\n","    return x/std_x, std_x\n","\n","def sampling(args):\n","\n","\t\"\"\"Sample the latent variable\n","\tfrom a Normal distribution.\"\"\"\n","\n","\ts_mean= args\n","\tepsilon = K.random_normal(shape=(batch_size,rc_dim), mean=0.0, stddev=s_vari )\n","\ts_noise = s_mean +  epsilon\n","\n","\treturn s_noise\n","\n","def dynamic_correction_loss(x, w1, w2):\n","    \"\"\"Custom loss function with dynamic correction\n","       \n","       Parameters:\n","       -----------\n","       x: input data\n","       w1:\n","       w2:\n","       \"\"\"\n","\n","    def custom_loss(y_true, y_pred ):\n","         ce1 = mean_squared_error(y_true, y_pred )\n","         ce2 = mean_squared_error(x, y_pred)  \n","         return (w1[:,0]*ce1+w2[:,0]*ce2) \n","\n","    return custom_loss\n","\n","class WeightsHistory(Callback):\n","\n","    def on_train_begin(self, logs={}):\n","        self.losses = []\n","        self.losses_vali = []\n","        self.weights0 = []\n","\n","    def on_epoch_end(self, epoch, logs={}):\n","        self.losses.append(logs.get('loss'))\n","        self.losses_vali.append(logs.get('val_loss'))\n","        self.weights0.append( prave.layers[1].get_weights()) # Coefficients of first layer\n","\n","def COLVAR2npy(Name, Temperature, num_OPs_used, Dir ='input/', first_row = 0, Bias = False, n_bias_convert = 1):\n","    ''' \n","    Convert the COLVAR (file that contains the trajectory and corresponding biasing potential) to npy file.\n","    Here we assume that the COLVAR is of the shape: n rows * (1+d1+d2+d3+d4) columns, where:\n","        \n","        - n is the number of rows of data printed by plumed (depends on the total simulation time, time step and plumed stride)\n","        \n","        - the first column is the simulation time in ps\n","        - d1 is the number of order parameters that will be used to train the TAE\n","        - d2 is the number of order parameters that won't be used to train the TAE \n","        - d3 is the number of reaction coordinates\n","        - d4 is the number of bias potentials that are added during the simulation\n","\n","        where d3 might be different from d4 as we might have a 2D RC with Vbias(RCx, RCy)\n","        \n","        Parameters\n","        ----------\n","        Name : string\n","            Name of the system.\n","            \n","        Temperature : float\n","            Temperature in unit of Kelvin.\n","        \n","        num_OPs_used : int\n","            Dimensionality of order parameter space (feature space) used to train the TAE\n","        \n","        Dir: string\n","            Directory of input and output (.npy) files.\n","        \n","        first_row: int\n","            First row index to consider from the input dataset\n","\n","        Bias: Bool\n","            Whether the trajectory is from a biased MD.\n","            When false re-weighting factors are set to 1. \n","            When true, re-weighting factors are calculated and saved. \n","\n","        n_bias_convert: int\n","            (only used if bias = True) number of biases that will be converted into re-weighting factors, we might want to leave some\n","            without re-weighting to apply the \"washing out\" trick. Usually equal to the number of already learned RC linear components\n","    \n","        Returns\n","        -------\n","        None\n","        \n","    '''  \n","\n","    # If the traj is biased \n","    if Bias:\n","\n","        # Files to save data (x) and weights (w)\n","        traj_save_name = Dir + 'x_' + Name\n","        bias_save_name = Dir + 'w_' + Name\n","\n","        if path.exists(traj_save_name + '.npy') and path.exists(bias_save_name + '.npy'):\n","            print( 'npy files already exist, delete them if you want to generate new ones.')\n","        else:\n","          \n","            # Load input data\n","            Data = np.loadtxt(Dir + Name)   \n","\n","            # x: time and the value of the order parameters for all frames\n","            x = Data[::, 1:num_OPs_used+1]  # all rows, columns of used OP\n","\n","            # w has the value of total bias for every frame - associated to a certain value of the RC and order parameters. That sample in OP space will carry a weight = exp(V/KbT)\n","            w = np.sum( Data[::, -n_bias_convert :], axis = 1)  # All rows, summing all columns corresponding to biases we want to re-weight\n","\n","            # only a part of the full trajectory will be saved\n","            x = x[first_row:, :]   # rows used\n","            w = w[first_row:]      # Total bias to re-weight at each time step / sample\n","\n","            # Calculate re-weighting factor for each sample\n","            re_weighting_factor = np.exp(w/(0.008319*Temperature))   # Here we assume that unit of bias is kJ\n","            \n","            # Save OPs\n","            np.save(Dir+'x_'+ Name, x)\n","            \n","            # Save re-weighting factors - not normalized\n","            np.save(Dir+'w_'+ Name, re_weighting_factor) \n","\n","    # If the traj is unbiased \n","    else: \n","\n","        # Files to save data (x) and weights (w)\n","        traj_save_name = Dir + 'x_unbiased_' + Name\n","        bias_save_name = Dir + 'w_unbiased_' + Name     \n","\n","        if path.exists(traj_save_name+'.npy') and path.exists(bias_save_name+'.npy'):\n","            print( 'npy files already exit, delete them if you want to generate new ones.')\n","        else:\n","\n","            # Load input data\n","            Data = np.loadtxt(Dir+ Name) \n","\n","            # x: time and the value of the order parameters for all frames\n","            x = Data[::, 1:num_OPs_used+1]     # all rows, columns of used OP\n","\n","            # only a part of the full trajectory will be used \n","            x = x[first_row:, :] \n","\n","            # trajectories are treated as unbiased - re-weighting factors = 1\n","            re_weighting_factor = np.ones( np.shape(x[:,0]))    \n","\n","            # Save OPs\n","            np.save(Dir+'x_unbiased_'+ Name, x) \n","\n","            # Save re-weighting factors\n","            np.save(Dir+'w_unbiased_'+ Name, re_weighting_factor)\n","    \n","def save_result(system_name, op_dim, time_delay, trials, save_path='output/'):\n","    ''' save final result (linear combinaton coefficients of OPs) to a txt file\n","        Parameters\n","        ----------\n","        system_name : string\n","            Name of the system.\n","            \n","        op_dim : int\n","            Dimensionality of order parameters.\n","        \n","        time_delay : int\n","            Predictive time delay.\n","        \n","        trials: list\n","            Indexes of all trials.\n","        \n","        save_path: string\n","            Directory of where the final result is saved.\n","            \n","        Returns\n","        ----------\n","            None  \n","            Result is saved to a txt file.\n","    \n","    '''\n","    weights = []\n","    for dt in time_delay:\n","        Loss = []\n","        Weights = []\n","        for trial in trials: \n","            save_dir = system_name+'_dt'+str(dt)+'_trial'+str(trial)+'.npy'\n","            Result_loss = np.load(save_path+'Loss_'+save_dir) \n","            Result_weights = np.load(save_path+'Weights_'+save_dir) \n","            Loss.append(np.average( Result_loss[-2:,-1] ))\n","            Weights.append( Result_weights[-1,:,:] )\n","        \n","        Weights = np.array( Weights )\n","        min_index = np.argmin(Loss)\n","        weights.append( Weights[min_index,:,:] )\n","    weights = np.array(weights)\n","    \n","    ###save weights vs. time delay###\n","    head = 'time_delay/MD_step  '\n","    number_rcs = np.shape(Weights)[-1]\n","    print('There are %i reaction coordinates'%number_rcs)\n","    for j in range(op_dim):\n","        head+='op%i  '%(j+1)\n","    for j in range(len(time_delay)):\n","         result_given_dt = np.concatenate((np.transpose( [[time_delay[j]]*number_rcs] ), np.transpose(weights[j,:,:])), axis =-1)\n","         try:\n","             final_result = np.concatenate((final_result, result_given_dt), axis=0)\n","         except:\n","             final_result = result_given_dt\n","            \n","    np.savetxt(save_path+'final_result_'+system_name+'.txt', final_result, header =head, newline='\\n')\n","    "]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":873446,"status":"ok","timestamp":1682442859350,"user":{"displayName":"Pablo Navarro","userId":"03312151943936382336"},"user_tz":-120},"id":"71FaXdgJYdlu","outputId":"0f540874-189a-4b6a-df0f-f8e8e8bbbe0b"},"outputs":[{"name":"stdout","output_type":"stream","text":["npy files already exit, delete them if you want to generate new ones.\n","npy files already exit, delete them if you want to generate new ones.\n","npy files already exit, delete them if you want to generate new ones.\n","npy files already exit, delete them if you want to generate new ones.\n","npy files already exit, delete them if you want to generate new ones.\n","npy files already exit, delete them if you want to generate new ones.\n","npy files already exit, delete them if you want to generate new ones.\n","npy files already exit, delete them if you want to generate new ones.\n","length of data: 15479992\n","number of order parameters: 12\n","min re-weighting factor: 1.000000\n","max re-weighting factor: 1.000000\n","5000000 data points are used in this training\n","Train on 5000000 samples, validate on 5000000 samples\n","Epoch 1/2\n","5000000/5000000 [==============================] - ETA: 0s - loss: 0.9511"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  updates = self.state_updates\n"]},{"name":"stdout","output_type":"stream","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r5000000/5000000 [==============================] - 43s 9us/sample - loss: 0.9511 - val_loss: 0.9113\n","Epoch 2/2\n","5000000/5000000 [==============================] - 41s 8us/sample - loss: 0.8950 - val_loss: 0.8789\n","!!!!\n","(2, 12, 1)\n","5000000 data points are used in this training\n","Train on 5000000 samples, validate on 5000000 samples\n","Epoch 1/2\n","5000000/5000000 [==============================] - 42s 8us/sample - loss: 0.9338 - val_loss: 0.8762\n","Epoch 2/2\n","5000000/5000000 [==============================] - 42s 8us/sample - loss: 0.8600 - val_loss: 0.8433\n","!!!!\n","(2, 12, 1)\n","5000000 data points are used in this training\n","Train on 5000000 samples, validate on 5000000 samples\n","Epoch 1/2\n","5000000/5000000 [==============================] - 42s 8us/sample - loss: 0.9167 - val_loss: 0.8275\n","Epoch 2/2\n","5000000/5000000 [==============================] - 40s 8us/sample - loss: 0.8059 - val_loss: 0.7888\n","!!!!\n","(2, 12, 1)\n","length of data: 15479984\n","number of order parameters: 12\n","min re-weighting factor: 1.000000\n","max re-weighting factor: 1.000000\n","5000000 data points are used in this training\n","Train on 5000000 samples, validate on 5000000 samples\n","Epoch 1/2\n","5000000/5000000 [==============================] - 40s 8us/sample - loss: 0.9183 - val_loss: 0.8464\n","Epoch 2/2\n","5000000/5000000 [==============================] - 40s 8us/sample - loss: 0.8292 - val_loss: 0.8145\n","!!!!\n","(2, 12, 1)\n","5000000 data points are used in this training\n","Train on 5000000 samples, validate on 5000000 samples\n","Epoch 1/2\n","5000000/5000000 [==============================] - 40s 8us/sample - loss: 0.8560 - val_loss: 0.7340\n","Epoch 2/2\n","5000000/5000000 [==============================] - 40s 8us/sample - loss: 0.7223 - val_loss: 0.7115\n","!!!!\n","(2, 12, 1)\n","5000000 data points are used in this training\n","Train on 5000000 samples, validate on 5000000 samples\n","Epoch 1/2\n","5000000/5000000 [==============================] - 44s 9us/sample - loss: 0.9335 - val_loss: 0.8747\n","Epoch 2/2\n","5000000/5000000 [==============================] - 41s 8us/sample - loss: 0.8582 - val_loss: 0.8433\n","!!!!\n","(2, 12, 1)\n","length of data: 15479968\n","number of order parameters: 12\n","min re-weighting factor: 1.000000\n","max re-weighting factor: 1.000000\n","5000000 data points are used in this training\n","Train on 5000000 samples, validate on 5000000 samples\n","Epoch 1/2\n","5000000/5000000 [==============================] - 39s 8us/sample - loss: 0.9400 - val_loss: 0.8734\n","Epoch 2/2\n","5000000/5000000 [==============================] - 40s 8us/sample - loss: 0.8481 - val_loss: 0.8279\n","!!!!\n","(2, 12, 1)\n","5000000 data points are used in this training\n","Train on 5000000 samples, validate on 5000000 samples\n","Epoch 1/2\n","5000000/5000000 [==============================] - 41s 8us/sample - loss: 0.9618 - val_loss: 0.9233\n","Epoch 2/2\n","5000000/5000000 [==============================] - 41s 8us/sample - loss: 0.9060 - val_loss: 0.8880\n","!!!!\n","(2, 12, 1)\n","5000000 data points are used in this training\n","Train on 5000000 samples, validate on 5000000 samples\n","Epoch 1/2\n","5000000/5000000 [==============================] - 39s 8us/sample - loss: 0.9147 - val_loss: 0.8390\n","Epoch 2/2\n","5000000/5000000 [==============================] - 38s 8us/sample - loss: 0.8204 - val_loss: 0.8021\n","!!!!\n","(2, 12, 1)\n","There are 1 reaction coordinates\n"]}],"source":["########################\n","### Global Variables ###\n","\n","# Paths   \n","save_path = '/content/gdrive/MyDrive/WORK/NostrumBioDiscovery/2022_ENSEM/RAVE/Data/output/'     # path to the directory that saves output files\n","input_dir = '/content/gdrive/MyDrive/WORK/NostrumBioDiscovery/2022_ENSEM/RAVE/Data/input/'\n","\n","# Simulation details\n","system_name = 'p53'                                       # name of the system. Input files should be named: \"system_name_i\" starting with i=0\n","n_trajs = 8                                               # number of trajectories in input folder  NOTE: automate with number of \"system_name_i\" files \n","T = 300                                                   # Temperature in unit of Kelvin \n","num_ops_used = 12                                         # number of order parameters that should be used to define the RC\n","first_row = 0                                             # initial row index from the data to consider for training (to exclude equilibration if any)\n","bias = False                                              # When false, reweigting factors are set to 1. When true, reweigting factors are calculated, saved and used\n","n_bias_convert = 1                                        # (only used if bias = True) number of biases that will be converted into re-weighting factors starting to count from the end column, \n","                                                          # we might want to leave some without re-weighting to apply the \"washing out\" trick, usually equal to the number of already learned RC linear components\n","# Predictive time delay        \n","time_delay= [1, 2, 4]                                        # predictive time delay/s to use (in number of samples) - 0.5 ps between samples\n","    \n","# Network variables\n","training_size = 5000000                                   # \"training_size\" data points will be randomly picked from the whole data set and used to do the training\n","validation_size = 5000000                                 # \"validation_size\" data points will be randomly picked from (the whole data set - training set) and used to do the validation \n","batch_size = 50000                                        # total number of training data point n should be a multiple of batch_size \n","epochs = 2                                                # number of epochs to train the model\n","rc_dim = 1                                                # dimensionality of the RC\n","nn_dim = 128                                              # number of cells in each layer of the decoder NN\n","s_vari = 0.005                                            # constant variance of the gaussian noise added before the decoder\n","learning_rate = 0.0002                                    # constant learning rate\n","decay = 0.0                                               # decay of learning rate\n","trials = range(3)                                         # number of independent trials to train the time-lagged autoencoder\n","random_uniform = RandomUniform(minval=-0.5, maxval=0.5) \n","set_constant = Constant(value = 0.5**0.5)\n","whiten_data = True                                        # If True, then normalize the order parameters (set mean 0 and variance = 1)\n","\n","######################### \n","### input preparation ###\n","for traj_index in range(n_trajs):\n","\n","    input_name = system_name+'_%i'%traj_index\n","\n","    # Convert each trajectory's COLVAR file to npy file\n","    COLVAR2npy(input_name, T, num_ops_used, input_dir, first_row, bias, n_bias_convert)\n","\n","# Update for npy file name\n","if not bias:\n","    system_name = 'unbiased_' + system_name       \n","\n","################################# \n","### set predictive time delay ###\n","for dt in time_delay:      \n","\n","    #####################################\n","    ### load the dataset, whiten data ###\n","    X, X_dt, W1, W2, scaling_factors = data_prep(system_name, n_trajs, dt, input_dir, whiten_data, num_ops_used)\n","\n","    ############################   \n","    ### run different trials ###  \n","    for trial in trials:   \n","\n","        ############################################  \n","        ### Variational Autoencoder architecture ###\n","        input_Data = Input(batch_shape=(batch_size, num_ops_used))  \n","        input_w1 = Input(shape=(1,))    \n","        input_w2 = Input(shape=(1,))  \n","        linear_encoder = Dense(rc_dim, activation=None, use_bias=None, kernel_regularizer=regularizers.l1(0.0), kernel_initializer='random_uniform', kernel_constraint = unit_norm(axis=0))(input_Data)\n","              \n","        s = Lambda(sampling)(linear_encoder)  # Custom layer with the gaussian sampling\n","        hidden_a = Dense(nn_dim, activation='elu', kernel_initializer='random_uniform')(s)\n","        hidden_b = Dense(nn_dim, activation='elu', kernel_initializer='random_uniform')(hidden_a)\n","        y_reconstruction = Dense( num_ops_used, activation=None, kernel_initializer='random_uniform')(hidden_b)\n","        \n","        ##########################################\n","        ### Randomly pick samples from dataset ###\n","        train_x, train_xdt, train_w1, train_w2, vali_x, vali_xdt, vali_w1, vali_w2 = random_pick(X, X_dt, W1, W2, training_size, validation_size)\n","      \n","        #############################################\n","        ### Prepare the PRAVE and train the PRVAE ###\n","\n","        # Put the model together\n","        prave = Model([input_Data, input_w1 , input_w2] ,y_reconstruction)            \n","        \n","        # Create optimizer class\n","        rmsprop = RMSprop(learning_rate=learning_rate, decay = decay)\n","        \n","        # Compile the model\n","        prave.compile(optimizer=rmsprop,loss=dynamic_correction_loss(input_Data, input_w1, input_w2))\n","        \n","        # Create class \"WeightsHistory\"\n","        history = WeightsHistory()\n","\n","        # Fit the model, save loss and validation loss on epoch end\n","        History = prave.fit( [train_x,train_w1,train_w2], train_xdt,\n","          shuffle=True,\n","          epochs=epochs,\n","          batch_size=batch_size,\n","            validation_data=([vali_x,vali_w1,vali_w2], vali_xdt),\n","            callbacks = [history])\n","                \n","        ####################\n","        ### Save results ###\n","        Loss = np.array( history.losses )\n","        Val_Loss = np.array( history.losses_vali )     \n","\n","        # Coefficients of first layer (linear encoder)    \n","        Weights0 = np.array( history.weights0 )[:,0,:,:]  #  num epochs x ? x dim OP x dim latent space (num RC)\n","        \n","        # w_norm = np.linalg.norm(Weights0,  axis=1)\n","        for op_index in range( num_ops_used ):\n","            Weights0[:,op_index,:] /= scaling_factors[op_index] # rescale back to RC weights of non-standardized OPs\n","\n","        for rc_index in range( rc_dim ):\n","            Weights0[:, :, rc_index] = np.transpose( np.transpose( Weights0[:, :, rc_index] ) / np.linalg.norm(Weights0[:, :, rc_index], axis=1)) #normalize the rc weights\n","            \n","        Loss = np.expand_dims(Loss, axis=-1)\n","        Val_Loss = np.expand_dims(Val_Loss, axis=-1)\n","\n","        result_loss = np.concatenate((Loss, Val_Loss) , axis =-1)\n","\n","        result_weights = Weights0         \n","\n","        K.clear_session()\n","\n","        print('!!!!')\n","        print(np.shape(result_weights))\n","        \n","        # Name for results - to avoid overwriting different dt and trials for a certain set of hyper-parameters\n","        save_info = system_name+'_dt'+str(dt)+'_trial'+str(trial) \n","\n","        # Save loss and coefficients\n","        np.save(save_path+'Loss_'+save_info, result_loss)\n","        np.save(save_path+'Weights_'+save_info, result_weights)\n","\n","        del History\n","\n","    del X, X_dt, W1, W2, scaling_factors\n","\n","# Hyper-parameters \n","hyper_param={  \n","    \"train_size\": training_size,\n","    \"batch_size\": batch_size,\n","    \"epochs\": epochs,\n","    \"trials\": trial+1,\n","    \"layer_dimension\": nn_dim,\n","    \"learning_rate\": learning_rate,\n","    \"decay\": decay,\n","    \"decoder_noise_variance\": s_vari,\n","    \"num_dt_values\": len(time_delay),\n","    \"rc_dim\": rc_dim\n","} \n","  \n","# Dump hyperparameters  \n","json_object = json.dumps(hyper_param, indent = 4)\n","\n","# Writing to sample.json\n","json_file = save_path + \"hyper_parameters.json\"\n","with open(json_file, \"w\") as outfile:\n","    outfile.write(json_object)\n","\n","### analyze and save the results ###\n","save_result(system_name, num_ops_used, time_delay, trials, save_path)  \n"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOgFy0DXJt48nu4feNynyUZ","mount_file_id":"1RrO9hs_THd9AOn1lfUOrkA1wpz30lorY","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
